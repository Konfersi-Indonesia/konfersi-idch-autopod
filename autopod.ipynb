{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Dependency and Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pip requirements.txt\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Config and Dependency\n",
    "\n",
    "from yaml import safe_load\n",
    "from types import SimpleNamespace\n",
    "import requests as re\n",
    "import pandas as pd\n",
    "import os\n",
    "from cryptography.hazmat.primitives.asymmetric import rsa\n",
    "from cryptography.hazmat.primitives import serialization\n",
    "from cryptography.hazmat.backends import default_backend\n",
    "import base64\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import re as regex\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def substitute_env_variables(yaml_content):\n",
    "    \"\"\"Substitute environment variables in the YAML content.\"\"\"\n",
    "    # Regular expression to match ${VAR} or $VAR\n",
    "    pattern = regex.compile(r'\\${(.*?)}|\\$(\\w+)')\n",
    "    \n",
    "    def replace(match):\n",
    "        # Get the environment variable name from the match\n",
    "        env_var = match.group(1) or match.group(2)\n",
    "        # Return the value of the environment variable, or the original text if not found\n",
    "        res = os.environ.get(env_var, match.group(0))\n",
    "        return res\n",
    "    \n",
    "    # Replace environment variable placeholders with actual values\n",
    "    return pattern.sub(replace, yaml_content)\n",
    "\n",
    "def map_to_namespace(mapping):\n",
    "    \"\"\"\n",
    "    Convert a mapping (like a dictionary or map object) into a nested namespace.\n",
    "    \"\"\"\n",
    "    if isinstance(mapping, dict):  # If the object is a dictionary\n",
    "        return SimpleNamespace(**{key: map_to_namespace(value) for key, value in mapping.items()})\n",
    "    elif isinstance(mapping, (list, tuple)):  # For lists or tuples, apply recursively\n",
    "        return [map_to_namespace(item) for item in mapping]\n",
    "    else:  # Base case: return as is\n",
    "        return mapping\n",
    "\n",
    "def load_config(path):\n",
    "    \"\"\"Load and parse a YAML config file with environment variable substitution.\"\"\"\n",
    "    # Load environment variables from a .env file if available\n",
    "    load_dotenv()\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        # Read the file content\n",
    "        yaml_content = file.read()\n",
    "        # Substitute environment variables in the content\n",
    "        yaml_content = substitute_env_variables(yaml_content)\n",
    "        # Now parse the YAML content after substitution\n",
    "        return safe_load(yaml_content)\n",
    "\n",
    "\n",
    "def generate_ssh_key_pair(key_name=\"id_rsa\", key_size=2048):\n",
    "    \"\"\"\n",
    "    Generate an RSA SSH key pair and save them as files.\n",
    "\n",
    "    Args:\n",
    "        key_name (str): The base name of the key files (default is 'id_rsa').\n",
    "        key_size (int): The size of the RSA key in bits (default is 2048).\n",
    "    \"\"\"\n",
    "    # Generate the private key\n",
    "    private_key = rsa.generate_private_key(\n",
    "        public_exponent=65537,\n",
    "        key_size=key_size,\n",
    "        backend=default_backend()\n",
    "    )\n",
    "\n",
    "    # Serialize and save the private key\n",
    "    private_key_path = f\"{key_name}\"\n",
    "    with open(private_key_path, \"wb\") as priv_file:\n",
    "        priv_file.write(\n",
    "            private_key.private_bytes(\n",
    "                encoding=serialization.Encoding.PEM,\n",
    "                format=serialization.PrivateFormat.TraditionalOpenSSL,\n",
    "                encryption_algorithm=serialization.NoEncryption()\n",
    "            )\n",
    "        )\n",
    "    print(f\"Private key saved to: {private_key_path}\")\n",
    "\n",
    "    # Generate the public key\n",
    "    public_key = private_key.public_key()\n",
    "\n",
    "    # Serialize and save the public key in OpenSSH format\n",
    "    public_key_path = f\"{key_name}.pub\"\n",
    "    with open(public_key_path, \"wb\") as pub_file:\n",
    "        pub_file.write(\n",
    "            public_key.public_bytes(\n",
    "                encoding=serialization.Encoding.OpenSSH,\n",
    "                format=serialization.PublicFormat.OpenSSH\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fix_private_key_permissions(public_key_path)\n",
    "    print(f\"Public key saved to: {public_key_path}\")\n",
    "    return public_key_path\n",
    "\n",
    "def build_write_file_cloud(file, permissions = \"0755\", encoding = \"b64\", path=\"/home/ubuntu\", ):\n",
    "    target_path = os.path.join(path, os.path.basename(file))\n",
    "    \n",
    "    # Read the content of the bash file\n",
    "    with open(file, \"r\") as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    encoded_content = base64.b64encode(content.encode('utf-8')).decode('utf-8')\n",
    "    # Add to write_files\n",
    "    return {\n",
    "        \"path\": target_path,\n",
    "        \"content\": encoded_content,\n",
    "        \"permissions\": permissions,  # Ensuring the script is executable\n",
    "        \"encoding\": encoding\n",
    "    }\n",
    "\n",
    "def cloud_init_writer(path=\"/home/ubuntu\", files=[], bash_files=[], environments={}):\n",
    "    write_files = []\n",
    "    runcmd = []\n",
    "    \n",
    "    # Write files section\n",
    "    for file in files:\n",
    "        write_files.append(build_write_file_cloud(file, path=path))\n",
    "    \n",
    "    # Environment variables section\n",
    "    for key, value in environments.items():\n",
    "        runcmd.append(f\"export {key}='{value}'\")\n",
    "    \n",
    "    # Bash files section\n",
    "    for bash_file in bash_files:\n",
    "        # Get the filename from the full file path\n",
    "        filename = os.path.basename(bash_file)\n",
    "        target_path = os.path.join(path, filename)\n",
    "        log_file = f\"/var/log/{filename}.log\"  # Log file to store output\n",
    "        \n",
    "        # Add to runcmd\n",
    "        runcmd.extend([\n",
    "            f\"echo 'Running script: {filename}' >> /var/log/cloud-init.log\",\n",
    "            f\"chown ubuntu:ubuntu {target_path}\",\n",
    "            f\"chmod +x {target_path}\",\n",
    "            # Run the script and capture output (both stdout and stderr) to log file\n",
    "            f\"{target_path} >> {log_file} 2>&1\",  # Redirect both stdout and stderr\n",
    "            f\"echo 'Script {filename} execution complete' >> {log_file}\"\n",
    "        ])\n",
    "    \n",
    "    return {\n",
    "        \"write_files\": write_files,\n",
    "        \"runcmd\": runcmd\n",
    "    }\n",
    "\n",
    "def cloud_init_generator(files, runcmd, path=\"/home/ubuntu\", environments = {}):\n",
    "    if not isinstance(files, list):\n",
    "        folder_path = files\n",
    "        files = [os.path.join(files, f) for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    return json.dumps(cloud_init_writer(path, files, runcmd, environments=environments))\n",
    "\n",
    "config = map_to_namespace(load_config(\"config.yaml\"))\n",
    "print(\"Config Loaded:\", config)\n",
    "\n",
    "idch_header = {\n",
    "    \"apikey\": config.idch.token\n",
    "}\n",
    "\n",
    "def idch_get(path):\n",
    "    url = os.path.join(config.idch.host, path.format(location=config.cluster.location))\n",
    "    print('GET ' + url)\n",
    "    return pd.DataFrame(re.get(url, headers=idch_header).json())\n",
    "\n",
    "def idch_post(path, data):\n",
    "    url = os.path.join(config.idch.host, path.format(location=config.cluster.location))\n",
    "    print('POST ' + url)\n",
    "    res = re.post(url, headers=idch_header, data=data).json()\n",
    "    return pd.DataFrame(res)\n",
    "    \n",
    "def idch_delete(path, data=None):\n",
    "    url = os.path.join(config.idch.host, path.format(location=config.cluster.location))\n",
    "    print('DELETE ' + url)\n",
    "    print(re.delete(url, headers=idch_header, data=data).json())\n",
    "\n",
    "def idch_delete_instance(uuid, ip_address):\n",
    "    if (uuid):\n",
    "        print(\"Deleting Container:\", uuid)\n",
    "        idch_delete(\"v1/{location}/user-resource/vm\", data={\n",
    "            \"uuid\": uuid,\n",
    "        })\n",
    "    \n",
    "    if (ip_address):\n",
    "        print(\"Deleting IP:\", ip_address)\n",
    "        idch_delete(\"v1/{location}/network/ip_addresses/\" + ip_address)\n",
    "\n",
    "\n",
    "def idch_get_instances():\n",
    "    # Get the data from the API or external service\n",
    "    vm_list = idch_get(\"v1/{location}/user-resource/vm/list\")\n",
    "    ip_addresses = idch_get(\"v1/{location}/network/ip_addresses\")\n",
    "\n",
    "    if (len(ip_addresses) == 0):\n",
    "        print(vm_list)\n",
    "        return vm_list\n",
    "    \n",
    "    ip_addresses = ip_addresses.rename(columns={'address': 'public_ipv4', \"uuid\": \"network_uuid\"})\n",
    "    \n",
    "    if (len(vm_list) == 0):\n",
    "        return ip_addresses\n",
    "    \n",
    "\n",
    "    # Step 1: Perform the join on uuid and assigned_to\n",
    "    merged_df = pd.merge(vm_list, ip_addresses, left_on='uuid', right_on='assigned_to', how='inner')\n",
    "\n",
    "    # Step 2: Filter rows where the 'name' column starts with config.cluster.name\n",
    "    filtered_df = merged_df[merged_df['name'].str.startswith(config.cluster.name)]\n",
    "\n",
    "    # Step 3: Create a new DataFrame to ensure we aren't working on a slice of the original DataFrame\n",
    "    result = filtered_df[['uuid','network_uuid','name', 'private_ipv4', 'status', 'public_ipv4']].copy()\n",
    "\n",
    "    # Step 4: Add the 'command' column using .loc to avoid SettingWithCopyWarning\n",
    "    result.loc[:, 'command'] = result.apply(\n",
    "        lambda row: f\"ssh -i {config.cluster.keypair.private} {config.cluster.username}@{row['public_ipv4']}\",\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return result\n",
    "\n",
    "def convert_to_dict(key_value_list):\n",
    "    result = {}\n",
    "    for item in key_value_list:\n",
    "        key, value = item.split(\"=\", 1)  # Split at the first '=' character\n",
    "        result[key] = value\n",
    "    return result\n",
    "\n",
    "def idch_build_node(node_config, resource_config, role=\"master\", environments = {}):\n",
    "    with open(config.cluster.keypair.public, \"r\") as file:\n",
    "        public_key = file.read()\n",
    "        \n",
    "    if (node_config.cloud_init.environments):\n",
    "        environments.update(convert_to_dict(node_config.cloud_init.environments))\n",
    "\n",
    "    environments[\"CLOUD_INIT_WORKDIR\"] = node_config.cloud_init.path\n",
    "    environments[\"NODE_ROLE\"] = role\n",
    "    environments[\"NODE_USER\"] = config.cluster.username\n",
    "\n",
    "    data = {\n",
    "        \"name\": config.cluster.name + \"_master\",\n",
    "        \"os_name\": node_config.os_name,\n",
    "        \"os_version\": node_config.os_version,\n",
    "        \"disks\": int(resource_config.storage),\n",
    "        \"vcpu\": int(resource_config.cpu),\n",
    "        \"ram\": int(resource_config.memory) * (2 ** 10),\n",
    "        \"username\": config.cluster.username,\n",
    "        \"password\": config.cluster.password,\n",
    "        \"public_key\": public_key,\n",
    "        \"cloud_init\": cloud_init_generator(node_config.cloud_init.files, node_config.cloud_init.runcmd, node_config.cloud_init.path, environments=environments)\n",
    "    }\n",
    "\n",
    "    print(data)\n",
    "\n",
    "    return idch_post(\"v1/{location}/user-resource/vm\", data)\n",
    "\n",
    "# Assuming you have your instances DataFrame ready\n",
    "def idch_healthcheck_instance():\n",
    "    instances = idch_get_instances()  # Assume this returns a DataFrame with instance details\n",
    "    health_set = {}\n",
    "    count = 0\n",
    "\n",
    "    # Initialize tqdm progress bar for the loop\n",
    "    with tqdm(total=len(instances), desc=\"Checking health\", unit=\"instance\") as pbar:\n",
    "        while len(health_set.keys()) != len(instances):\n",
    "            count += 1\n",
    "            for i, row in instances.iterrows():\n",
    "                try:\n",
    "                    re.get(\"http://\" + row.get('public_ipv4') + \":8000/health\", timeout=1)\n",
    "                    health_set[row.get('uuid')] = True\n",
    "                    pbar.update(1)\n",
    "                except Exception as _:\n",
    "                    # Log to tqdm (not interrupting the progress bar)\n",
    "                    pbar.set_postfix({\"Error\": f\"Failed {row.get('uuid')}\", \"Iter\": count}, refresh=True)\n",
    "            \n",
    "\n",
    "def fix_private_key_permissions(private_key_path):\n",
    "    \"\"\"\n",
    "    This function ensures that the private key file has the correct permissions (600).\n",
    "    This is typically required for SSH private keys to ensure that only the owner can read/write it.\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(private_key_path):\n",
    "        raise FileNotFoundError(f\"The specified private key file does not exist: {private_key_path}\")\n",
    "    \n",
    "    # Check the current file permissions\n",
    "    current_permissions = oct(os.stat(private_key_path).st_mode)[-3:]\n",
    "    \n",
    "    # Set the correct permissions (600)\n",
    "    if current_permissions != '600':\n",
    "        print(f\"Fixing permissions for {private_key_path}. Current permissions: {current_permissions}\")\n",
    "        os.chmod(private_key_path, 0o600)  # Set permission to 600 (read/write for owner only)\n",
    "        print(f\"Permissions fixed to 600 for {private_key_path}\")\n",
    "    else:\n",
    "        print(f\"Permissions for {private_key_path} are already correctly set to 600.\")\n",
    "\n",
    "private_key_path = config.cluster.keypair.private\n",
    "\n",
    "if private_key_path:\n",
    "    try:\n",
    "        # Attempt to fix the permissions if the key exists\n",
    "        fix_private_key_permissions(private_key_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"Private key pair not found, generating one...\")\n",
    "        \n",
    "        # Extract the directory and file name from the private key path\n",
    "        key_dir = os.path.dirname(private_key_path)\n",
    "        key_name = os.path.basename(private_key_path)\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(key_dir, exist_ok=True)\n",
    "        \n",
    "        # Generate SSH key pair\n",
    "        generate_ssh_key_pair(private_key_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of Locations\n",
    "idch_get(\"v1/config/locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Available Network ID\n",
    "idch_get(\"v1/{location}/network/networks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Available OS Catalogue\n",
    "plain_oses = idch_get(\"v1/config/vm_images/plain_os\")[['os_name', 'versions']].explode('versions')\n",
    "plain_oses['os_version'] = plain_oses['versions'].apply(lambda x: x['os_version'])\n",
    "del plain_oses['versions']\n",
    "plain_oses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Master Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Master Node\n",
    "idch_build_node(config.master, config.master.init_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SSH Command\n",
    "idch_get_instances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Healthcheck\n",
    "idch_healthcheck_instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idch_build_node(config.worker, config.worker.resource, environments={\n",
    "    \"MASTER_NODE_IP\": \"192.168.0.1\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health Checker and Links\n",
    "idch_healthcheck_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Cluster\n",
    "for index, row in idch_get_instances().iterrows():\n",
    "    idch_delete_instance(row.get('uuid'), row.get('public_ipv4'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
